{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac7da674",
   "metadata": {},
   "source": [
    "## Downscaling-of-Precipitation-Reanalysis-Data-using-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6455e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "from scipy import stats\n",
    "import h5netcdf\n",
    "import tensorflow\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras import initializers\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import concatenate\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import gc\n",
    "gc.disable()\n",
    "\n",
    "\n",
    "def series_to_supervised(df, n_in=1, n_out=1, dropnan=True):\n",
    "        n_vars = 1 if type(df) is list else df.shape[1]\n",
    "        df = pd.DataFrame(df)\n",
    "        cols, names = list(), list()\n",
    "        # input sequence (t-n, ... t-1)\n",
    "        for i in range(n_in, 0, -1):\n",
    "                cols.append(df.shift(i))\n",
    "                names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "        # forecast sequence (t, t+1, ... t+n)\n",
    "        for i in range(0, n_out):\n",
    "                cols.append(df.shift(-i))\n",
    "                if i == 0:\n",
    "                        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "                else:\n",
    "                        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "        # put it all together\n",
    "        agg = pd.concat(cols, axis=1)\n",
    "        agg.columns = names\n",
    "        # drop rows with NaN values\n",
    "        if dropnan:\n",
    "                agg.dropna(inplace=True)\n",
    "        return agg\n",
    "\n",
    "ncep='/home/ramesh/bias/Final_Programs/JJAS.nc'\n",
    "imd='/home/ramesh/bias/Final_Programs/JJAS_IMD.nc'\n",
    "mask1 = '/nmhs1/reanalysis/agromet/reanalysis/inter/mask_fin.nc'\n",
    "#ncep_imd = '/nmhs1/reanalysis/agromet/reanalysis/inter/rf_dly_ncep_imd1.nc'\n",
    "ncep_ds = xr.open_dataset(ncep)\n",
    "imd_ds=xr.open_dataset(imd)\n",
    "\n",
    "cent=0\n",
    "\n",
    "#data_ds = data_ds.to_array()\n",
    "mask = xr.open_dataset(mask1)\n",
    "mask = mask['MASK']\n",
    "imd_ds= imd_ds['RF']\n",
    "ncep_ds=ncep_ds['prate']\n",
    "\n",
    "rmse_train = mask\n",
    "mae_train = mask\n",
    "rmse_test = mask\n",
    "mae_test = mask\n",
    "rmse_comb = mask\n",
    "mae_comb = mask\n",
    "\n",
    "lon= ncep_ds['XFINE']\n",
    "#lon=lon[1:59]\n",
    "latt=ncep_ds['YFINE']\n",
    "#latt=latt[1:59]\n",
    "times=ncep_ds['time'].values\n",
    "np.savetxt('times_final.csv',times)\n",
    "#times=pd.Series(times,index=None)\n",
    "\n",
    "\n",
    "#print(\"MASK DATA\",mask)\n",
    "#print('ncep',ncep_ds)\n",
    "\n",
    "#print(data_ds)\n",
    "\n",
    "#print('at x , y :\\n',data_ds[1,125,2,3])\n",
    "#print('shape',data_ds.shape)\n",
    "\n",
    "#k = mask.shape[2]\n",
    "#l = mask.shape[3]\n",
    "#a = ncep_ds.shape[1]\n",
    "#print('a',a,'l',l,'k',k)\n",
    "#count =0 \n",
    "lati=ncep_ds.shape[1]\n",
    "loni=ncep_ds.shape[2]\n",
    "tt=ncep_ds.shape[0]\n",
    "#print('lati',lati,'loni',loni,'time',tt)\n",
    "#np.savetxt('time.csv',ncep_ds['time'])\n",
    "p = pd.DataFrame([])\n",
    "for j in range(loni):\n",
    "    for i in range(lati):\n",
    "        #print('running',i,j)\n",
    "        if mask[0,i,j] == 1:\n",
    "            \n",
    "            ncep_i = ncep_ds[:,i,j]\n",
    "            times=ncep_i['time'].values\n",
    "            ncep_i=ncep_i.values\n",
    "            #ncep_i=pd.Series(ncep_i,index=None)\n",
    "            #print('ncep',ncep_i)\n",
    "            imd_i =imd_ds[:,i,j].values\n",
    "            #imd_i=pd.Series(imd_i,index=None)\n",
    "            #print('imd',imd_i)\n",
    "            lon_i=lon[j].values\n",
    "            lat_i=latt[i].values\n",
    "            #print('TIME',times)\n",
    "            \n",
    "            df=pd.DataFrame(data={'Variable':'RF','TFINE':times,'XFINE':lon_i ,'YFINE':lat_i,'rf':imd_i})\n",
    "            #print('Dataframe',df)\n",
    "            df1=pd.DataFrame(data={'Variable':'RFNCEP','TFINE':times,'XFINE':lon_i,'YFINE':lat_i,'rf':ncep_i})\n",
    "            #print('Dataframe_NCEP',df1)\n",
    "\n",
    "\n",
    "            data= pd.concat([df,df1])\n",
    "            #print('Dataframe_Concat:',data)\n",
    "\n",
    "            #rf = data_ds.sel(variable='RF',XFINE1=i,YFINE1=j)\n",
    "            #data = rf.to_dataframe(\"rf\")\n",
    "            #print(rf)\n",
    "\n",
    "            ns = int(data.shape[0]/2)\n",
    "            dfc1 = data[:ns ]\n",
    "            dfc2 = data[ns:]\n",
    "            #df = pd.merge(dfc1,dfc2, on = [\"TFINE\"])\n",
    "            \n",
    "            df = pd.merge(dfc2,dfc1, on = [\"TFINE\"])\n",
    "            df.reset_index(drop=True,inplace=True)\n",
    "            newdf = df.drop(['Variable_x','Variable_y','XFINE_x','XFINE_y','YFINE_x','YFINE_y'],axis=1)\n",
    "            #newdf =newdf.rename(columns={'rf_x': 'ncep','rf_y':'imd'})\n",
    "            #print('Prepared_data for LSTM : \\n', newdf, '\\n Data_shape\\n', newdf.shape)\n",
    "            newdf.reset_index(drop=True,inplace=True)\n",
    "            newdf.set_index('TFINE')\n",
    "            \n",
    "            feature = newdf[['rf_x','rf_y']]\n",
    "            #print('Feature',feature)\n",
    "            feature = feature.astype(float)\n",
    "            scaler = MinMaxScaler(feature_range=(0,1))\n",
    "            scaled = scaler.fit_transform(feature)\n",
    "            #print('MinMaxScaler Transformation: \\n',scaled)\n",
    "            \n",
    "            # Series to supervised learning\n",
    "            reframed = series_to_supervised(scaled, 1, 1)\n",
    "            # drop columns we don't want to predict\n",
    "            cd = scaled.shape[1]+1\n",
    "            reframed.drop(reframed.iloc[:, cd:], axis=1, inplace=True)\n",
    "            \n",
    "            # Train_test split\n",
    "            values = reframed.values\n",
    "            tn = int(values.shape[0]*0.8)\n",
    "            train = values[:tn]\n",
    "            test = values[tn:]\n",
    "            \n",
    "            # split into input and outputs\n",
    "            train_X, train_y = train[:, :-1], train[:, -1]\n",
    "            test_X, test_y = test[:, :-1], test[:, -1]\n",
    "            # reshape input to be 3D [samples, timesteps, features]\n",
    "            train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "            test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "            #print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "\n",
    "            # LSTM model Hyperparametrs\n",
    "            nodes = [50]\n",
    "            dr_ly = [0.20]\n",
    "            #optim = ['Adam']\n",
    "            opt = tensorflow.keras.optimizers.Adam(learning_rate=0.1)\n",
    "            optim=[opt]\n",
    "            activ = ['tanh']\n",
    "            epoch = [50]\n",
    "            b_s = [64]\n",
    "            input =(train_X.shape[1], train_X.shape[2])\n",
    "            train_in = train_X\n",
    "            train_out = train_y\n",
    "            test_in = test_X\n",
    "            test_out = test_y\n",
    "        \n",
    "            initializer = tensorflow.keras.initializers.Orthogonal()\n",
    "            #initializer = tensorflow.keras.initializers.GlorotNormal()\n",
    "            bias_initializer = tensorflow.keras.initializers.HeNormal()\n",
    "            #bias_initializer = 'Zeros'\n",
    "            for unit in nodes:\n",
    "                for dropout in dr_ly:\n",
    "                    for activation in activ:\n",
    "                        for optimizer in optim:\n",
    "                            for epochs in epoch:\n",
    "                                for batch in b_s:\n",
    "                                    model = Sequential()\n",
    "                                    model.add(Bidirectional(LSTM(unit, input_shape=input)))\n",
    "                                    model.add(Dropout(dropout))\n",
    "                                    model.add(Dense(1, kernel_initializer=initializer,use_bias=False, bias_initializer=bias_initializer))\n",
    "                                    model.add(Activation(activation))\n",
    "                                    model.compile(loss='mae', optimizer=optimizer, metrics = ['accuracy'])\n",
    "                                    history = model.fit(train_in, train_out, epochs=epochs, batch_size=batch, validation_data=(test_in, test_out), verbose=2, shuffle=False)\n",
    "\n",
    "                                    trhat = model.predict(train_X)\n",
    "                                    \n",
    "                                    train_X = train_X.reshape((train_X.shape[0], train_X.shape[2]))\n",
    "                                    inv_trhat = concatenate((trhat, train_X[:, 1:]), axis=1)\n",
    "                                    #print(inv_trhat)\n",
    "                                    inv_trhat = scaler.inverse_transform(inv_trhat)\n",
    "                                    inv_trhat = inv_trhat[:,0]\n",
    "                                    \n",
    "                                    train_y = train_y.reshape((len(train_y), 1))\n",
    "                                    inv_tny = concatenate((train_y, train_X[:, 1:]), axis=1)\n",
    "                                    inv_tny = scaler.inverse_transform(inv_tny)\n",
    "                                    inv_tny2 = inv_tny[:,0]\n",
    "                                    inv_tny1=inv_tny[:,1]\n",
    "                                    \n",
    "                                    prediction = pd.DataFrame(dict(actual = inv_tny1, predicted = inv_trhat)).reset_index()\n",
    "                                    #print(\"Train_Prediction :\\n\", prediction)\n",
    "                                    #prediction.to_csv(\"Train_data_AAAAA.csv\", index=False)\n",
    "                                    Train_data= prediction\n",
    "                                    Train_data[Train_data['predicted'] < 0] = 0\n",
    "                                    tr_rmse = sqrt(mean_squared_error(Train_data['actual'], Train_data['predicted']))\n",
    "                                    tr_mae = mean_absolute_error(Train_data['actual'], Train_data['predicted'])\n",
    "                                    #tr_mape =  np.mean(np.abs((Train_data['actual'] - Train_data['predicted']) / Train_data['actual'])) * 100\n",
    "                                    print('TRAIN_Data Errors','Tr_RMSE: %.3f' % tr_rmse,'Tr_MAE: %.3f' % tr_mae)#,'Tr_MAPE: %.3f' % tr_mape,)\n",
    "                                    #print('Latlon',lat_i,lon_i)\n",
    "\n",
    "                                    yhat = model.predict(test_X)\n",
    "                                    test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "                                    inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "                                    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "                                    inv_yhat = inv_yhat[:,0]\n",
    "                                    test_y = test_y.reshape((len(test_y), 1))\n",
    "                                    inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "                                    inv_y = scaler.inverse_transform(inv_y)\n",
    "                                    inv_y2 = inv_y[:,0]\n",
    "                                    inv_y1=inv_y[:,1]\n",
    "                                    forecast = pd.DataFrame(dict(actual = inv_y1, predicted = inv_yhat)).reset_index()\n",
    "                                    #print(\"\\nTest_Prediction:\\n\", forecast)\n",
    "                                    forecast.to_csv('forecast.csv')\n",
    "                                    Test_data = forecast\n",
    "                                    Test_data[Test_data['predicted']<0]=0\n",
    "                                    ts_rmse = sqrt(mean_squared_error(Test_data['actual'], Test_data['predicted']))\n",
    "                                    ts_mae = mean_absolute_error(Test_data['actual'], Test_data['predicted'])\n",
    "                                    #ts_mape =  np.mean(np.abs((Test_data['actual'] - Test_data['predicted']) / Test_data['actual'])) * 100\n",
    "                                    print('TEST_Data Errors','Ts_RMSE: %.3f' % ts_rmse,'Ts_MAE: %.3f' % ts_mae)#,'Ts_MAPE: %.3f' % ts_mape)\n",
    "\n",
    "                                    n = Train_data.shape[0]\n",
    "                                    d = np.abs(  np.diff( Train_data['actual']) ).sum()/(n-1)\n",
    "                                    errors = np.abs(Test_data['actual'] - Test_data['predicted'] )\n",
    "                                    MASE = errors.mean()/d\n",
    "                                    combined_data = Train_data.append(Test_data)\n",
    "                                    print('combined_data',combined_data)\n",
    "                                    \n",
    "\n",
    "                                    rmse = sqrt(mean_squared_error(combined_data['actual'], combined_data['predicted']))\n",
    "                                    mae = mean_absolute_error(combined_data['actual'], combined_data['predicted'])\n",
    "                                    #mape = np.mean(np.abs((combined_data['actual'] - combined_data['predicted']) / combined_data['actual'])) * 100\n",
    "                                    print('combined_data Errors','RMSE: %.3f' % rmse,'MAE: %.3f' % mae,'MASE: %.4f' % MASE)\n",
    "                                    \n",
    "                                    \n",
    "                                    file='ncep_pred'+str(i)+str(j)\n",
    "                                    \n",
    "                                    cent=cent+1\n",
    "                                    #print(tt)\n",
    "                                    import pickle\n",
    "\n",
    "                                    ds = xr.Dataset(\n",
    "                                            data_vars=dict(ncep_act=([\"time\"], ncep_i[:-1]),\n",
    "                                                   ncep_pred=([\"time\"],combined_data['predicted'].values),\n",
    "                                                   imd_act=(['time'],combined_data['actual'].values),\n",
    "                                                 ),\n",
    "            \n",
    "                                    coords=dict(\n",
    "                                        time=times[:-1],\n",
    "                                        center=cent,\n",
    "                                             ),\n",
    "                                    attrs=dict(\n",
    "                                        description=\"NCEP and IMD actual and predicted values .\",\n",
    "                                        center=[lat_i,lon_i],\n",
    "                                            ),\n",
    "                                                 )\n",
    "                                    \n",
    "                                   \n",
    "                                    ds.to_netcdf('NCEP_PRED/'+file+'.nc')\n",
    "\n",
    "\n",
    "\n",
    "                                    '''\n",
    "                                    outfile=open('NCEP_PRED/'+file+'.pkl','wb')\n",
    "                                    pickle.dump(ds,outfile)\n",
    "                                    outfile.close()\n",
    "\n",
    "                                    \n",
    "                                    ncep_pred[:,i,j]=combined_data['predicted'].values\n",
    "                                    imd_act[:,i,j]=combined_data['actual'].values\n",
    "                                    ncep_act[:,i,j]=ncep_i.values\n",
    "                                    '''\n",
    "                                    gc.collect()\n",
    "                                    \n",
    "\n",
    "\n",
    "                                    \n",
    "            rmse_test[0,i,j]= ts_rmse\n",
    "            mae_test[0,i,j]= ts_mae\n",
    "            rmse_train[0,i,j]=tr_rmse\n",
    "            mae_train[0,i,j]=ts_rmse\n",
    "            rmse_comb[0,i,j]=rmse\n",
    "            mae_comb[0,i,j]=mae\n",
    "            #print('\\nrmse and mae @ i, j\\n',newarray[0:0:i,j])\n",
    "            #input(\"Enter to continue\")\n",
    "            \n",
    "        else:\n",
    "            rmse_train[0,i,j] = -999999\n",
    "            mae_train[0,i,j] =-999999\n",
    "            rmse_test[0,i,j] = -999999\n",
    "            mae_test[0,i,j] =-999999\n",
    "            rmse_comb[0,i,j] = -999999\n",
    "            mae_comb[0,i,j] =-999999\n",
    "            #print(newarray[0,0,i,j])\n",
    "        #input(\"Enter to continue\")\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rmse_test.to_netcdf(\"rmse_test1.nc\")\n",
    "mae_test.to_netcdf(\"mae_test1.nc\")\n",
    "\n",
    "rmse_train.to_netcdf(\"rmse_train1.nc\")\n",
    "mae_train.to_netcdf(\"mae_train1.nc\")\n",
    "\n",
    "rmse_comb.to_netcdf(\"rmse_comb1.nc\")\n",
    "mae_comb.to_netcdf(\"mae_comb1.nc\")\n",
    "\n",
    "\n",
    "https://github.com/Hamsaask/Downscaling-of-Precipitation-Reanalysis-Data-using-LSTM\n",
    "\n",
    "#ncep_pred.to_netcdf('PREDICTED_NCEP_TRIAL.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
